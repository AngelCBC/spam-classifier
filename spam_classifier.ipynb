{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spam-classifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPXC2Y2vCc6p2sFcBkDDdHR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AngelCBC/spam-classifier/blob/main/spam_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzcdBkDrsNie",
        "outputId": "42a32ee5-e86e-48e8-f731-603d140de48f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd \"/content\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the urlextract module.\n",
        "\n",
        "!pip3 install urlextract"
      ],
      "metadata": {
        "id": "R2cQDfKQFDRf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "510b5034-51eb-443d-b166-34f9b0b12d16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting urlextract\n",
            "  Downloading urlextract-1.5.0-py3-none-any.whl (20 kB)\n",
            "Collecting uritools\n",
            "  Downloading uritools-3.0.2-py3-none-any.whl (12 kB)\n",
            "Collecting platformdirs\n",
            "  Downloading platformdirs-2.4.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from urlextract) (2.10)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from urlextract) (3.4.0)\n",
            "Installing collected packages: uritools, platformdirs, urlextract\n",
            "Successfully installed platformdirs-2.4.0 uritools-3.0.2 urlextract-1.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download some spamassassin datasets.\n",
        "\n",
        "import os, tarfile\n",
        "from urllib.request import urlretrieve\n",
        "\n",
        "DOWNLOAD_ROOT = \"http://spamassassin.apache.org/old/publiccorpus/\"\n",
        "\n",
        "FILE_NAMES = [\"20030228_easy_ham.tar.bz2\",\n",
        "            \"20030228_spam.tar.bz2\",\n",
        "            \"20021010_hard_ham.tar.bz2\",\n",
        "            \"20030228_easy_ham_2.tar.bz2\",\n",
        "            \"20030228_spam_2.tar.bz2\"]\n",
        "\n",
        "def fetch_data(download_url=DOWNLOAD_ROOT, file_names=FILE_NAMES):\n",
        "    for name in file_names:\n",
        "        db_url = download_url + name\n",
        "        urlretrieve(db_url, name)\n",
        "        tar_file = tarfile.open(name)\n",
        "        tar_file.extractall(path=os.getcwd())\n",
        "        tar_file.close()\n",
        "        os.remove(name)\n",
        "\n",
        "fetch_data()"
      ],
      "metadata": {
        "id": "LFwIjuFQs-g4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parse the emails.\n",
        "\n",
        "from glob import glob\n",
        "from email.policy import default\n",
        "from email.parser import BytesParser\n",
        "from os import listdir\n",
        "\n",
        "def parse_emails():\n",
        "    folder_paths = list(filter(lambda x: (\"spam\" in x) or (\"ham\" in x), listdir()))\n",
        "    email_list, type_list = list(), list()\n",
        "    for folder in folder_paths:\n",
        "        email_type = 1 if \"spam\" in folder else 0\n",
        "        email_paths = glob(folder + \"/*\")\n",
        "        for path in email_paths:\n",
        "            with open(path, \"rb\") as fp:\n",
        "                mail = BytesParser(policy=default).parse(fp)\n",
        "            email_list.append(mail)\n",
        "            type_list.append(email_type)\n",
        "    return email_list, type_list\n",
        "\n",
        "emails, labels = parse_emails()\n",
        "print(f\"{len(emails)} emails are included\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qk_O0YlML2NS",
        "outputId": "ad7b7a06-9777-4a4b-ce46-d27cce68145c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6051 emails are included\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training and testing split.\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(emails, labels,\n",
        "                                                    test_size=0.33, \n",
        "                                                    random_state=42,\n",
        "                                                    shuffle=True,\n",
        "                                                    stratify=labels)"
      ],
      "metadata": {
        "id": "dT2s732YH-MG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace urls with \"URL\".\n",
        "\n",
        "from urlextract import URLExtract\n",
        "\n",
        "def replace_urls(doc):\n",
        "    url_extractor = URLExtract()\n",
        "    urls = url_extractor.find_urls(doc)\n",
        "    for url in urls:\n",
        "        doc = doc.replace(url, \" URL \")\n",
        "    return doc"
      ],
      "metadata": {
        "id": "zKdtUh5QHIEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace numbers with \"NUM\".\n",
        "\n",
        "import re\n",
        "\n",
        "def replace_nums(doc):\n",
        "    filtered = re.sub(r'\\d+(?:\\.\\d*)?(?:[eE][+-]?\\d+)?', 'NUM', doc)\n",
        "    return filtered"
      ],
      "metadata": {
        "id": "miBldG60S4eR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize a certain word.\n",
        "\n",
        "import nltk, string\n",
        "\n",
        "def normalize_token(word):\n",
        "    translator = str.maketrans(\"\", \"\", string.punctuation) # delete punct\n",
        "    word_modif = word.translate(translator).lower() # lowercase\n",
        "    stemmer = nltk.PorterStemmer() # stemmer\n",
        "    return stemmer.stem(word_modif)"
      ],
      "metadata": {
        "id": "rjektEXYDf8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the vocabulary for one email. \n",
        "\n",
        "def build_vocabulary(doc):\n",
        "    doc_vocab = set()\n",
        "    for word in doc.split():\n",
        "        doc_vocab.add(normalize_token(word))\n",
        "    return doc_vocab"
      ],
      "metadata": {
        "id": "XJn_7IXDhWY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Whole vocabulary creation with the training set emails.\n",
        "\n",
        "whole_vocab = set()\n",
        "\n",
        "for ix, doc in enumerate(X_train):\n",
        "    try:\n",
        "        doc = replace_urls(doc.get_body().get_content())\n",
        "        doc = replace_nums(doc)\n",
        "        doc_vocab = build_vocabulary(doc)\n",
        "        whole_vocab = whole_vocab.union(doc_vocab)\n",
        "    except:\n",
        "        # Delete bad candidates from the training set:\n",
        "        X_train.pop(ix)\n",
        "        y_train.pop(ix)\n",
        "\n",
        "# Get rid of common words:\n",
        "\n",
        "no_info_words = {\"\", \"from\", \"to\", \"a\", \"an\", \"is\", \"of\", \"and\", \"are\", \"the\"}\n",
        "whole_vocab = whole_vocab - no_info_words\n",
        "\n",
        "# Get rid of long words:\n",
        "\n",
        "filtered_vocab = [x for x in list(whole_vocab) if len(x) < 10]\n",
        "\n",
        "print(f\"The vocabulary consists of {len(filtered_vocab)} words\")"
      ],
      "metadata": {
        "id": "DX3yd_7RYDV6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f151e69-23da-40f4-cc81-63e950f3e43a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The vocabulary consists of 33348 words\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean the test set.\n",
        "\n",
        "for ix, doc in enumerate(X_test):\n",
        "    try:\n",
        "        doc = doc.get_body().get_content()\n",
        "    except:\n",
        "        X_test.pop(ix)\n",
        "        y_test.pop(ix)"
      ],
      "metadata": {
        "id": "ec_2YHRNlxFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Word X_train matrix creation.\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "word_matrix = {}\n",
        "for ix, doc in enumerate(X_train):\n",
        "    doc_counts = {}\n",
        "    doc = replace_urls(doc.get_body().get_content())\n",
        "    doc = replace_nums(doc)\n",
        "    doc_words = [normalize_token(x) for x in doc.split()]\n",
        "    for token in filtered_vocab:\n",
        "        doc_counts[token] = doc_words.count(token)\n",
        "    doc_name = \"doc_train {}\".format(ix)\n",
        "    word_matrix[doc_name] = doc_counts\n",
        "\n",
        "X_train_matrix = pd.DataFrame(word_matrix).T\n",
        "\n",
        "# Word X_test matrix creation.\n",
        "\n",
        "word_matrix = {}\n",
        "for ix, doc in enumerate(X_test):\n",
        "    doc_counts = {}\n",
        "    doc = replace_urls(doc.get_body().get_content())\n",
        "    doc = replace_nums(doc)\n",
        "    doc_words = [normalize_token(x) for x in doc.split()]\n",
        "    for token in filtered_vocab:\n",
        "        doc_counts[token] = doc_words.count(token)\n",
        "    doc_name = \"doc_test {}\".format(ix)\n",
        "    word_matrix[doc_name] = doc_counts\n",
        "\n",
        "X_test_matrix = pd.DataFrame(word_matrix).T"
      ],
      "metadata": {
        "id": "TJyUEk1LX0Nc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_train_matrix), len(y_train), len(X_test_matrix), len(y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbnQUCuAccUD",
        "outputId": "f4e214f4-a432-4f47-d91a-2842ef398ebd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4006, 4006, 1972, 1972)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Traing classification acc with LogisticRegression model and CV.\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "log_clf = LogisticRegression(max_iter=1000, random_state=42)\n",
        "score = cross_val_score(log_clf, X_train_matrix, y_train, cv=3, verbose=3)\n",
        "score.mean()"
      ],
      "metadata": {
        "id": "cM2qhItPs-oA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5568ad76-b623-4aa0-b531-500e6ccdafbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ................................ score: (test=0.974) total time= 3.2min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  3.3min remaining:    0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ................................ score: (test=0.973) total time=  41.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  3.9min remaining:    0.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CV] END ................................ score: (test=0.971) total time= 3.3min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  7.2min finished\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9725408733095606"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Precision and Recall computation of the test set classification.\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "log_clf = LogisticRegression(solver=\"lbfgs\", max_iter=1000, random_state=42)\n",
        "log_clf.fit(X_train_matrix, y_train)\n",
        "\n",
        "y_pred = log_clf.predict(X_test_matrix)\n",
        "\n",
        "print(\"Precision: {:.2f}%\".format(100 * precision_score(y_test, y_pred)))\n",
        "print(\"Recall: {:.2f}%\".format(100 * recall_score(y_test, y_pred)))"
      ],
      "metadata": {
        "id": "xA-mDn3xs-sN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "558ed8d1-4062-4c51-f7c2-b0cf415746f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 97.14%\n",
            "Recall: 95.85%\n"
          ]
        }
      ]
    }
  ]
}